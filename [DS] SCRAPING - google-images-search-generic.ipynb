{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:30:57.033716Z",
     "start_time": "2021-11-19T14:30:51.390754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver-binary in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (93.0.4577.63.0)\n",
      "Collecting chromedriver-binary\n",
      "  Downloading chromedriver-binary-97.0.4692.20.0.tar.gz (4.8 kB)\n",
      "Building wheels for collected packages: chromedriver-binary\n",
      "  Building wheel for chromedriver-binary (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chromedriver-binary: filename=chromedriver_binary-97.0.4692.20.0-py3-none-any.whl size=10135190 sha256=00a181a7792e7a284055fd5c186a66dd2b6e09efc2833dea56357b50b9c25707\n",
      "  Stored in directory: /home/christophearendt/.cache/pip/wheels/d5/d0/ff/4718060215c8c5181c85a13f7efebf5c849e616e14e4235bf8\n",
      "Successfully built chromedriver-binary\n",
      "Installing collected packages: chromedriver-binary\n",
      "  Attempting uninstall: chromedriver-binary\n",
      "    Found existing installation: chromedriver-binary 93.0.4577.63.0\n",
      "    Uninstalling chromedriver-binary-93.0.4577.63.0:\n",
      "      Successfully uninstalled chromedriver-binary-93.0.4577.63.0\n",
      "Successfully installed chromedriver-binary-97.0.4692.20.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: selenium in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (3.141.0)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
      "\u001b[K     |████████████████████████████████| 958 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
      "\u001b[K     |████████████████████████████████| 356 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from selenium) (1.26.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: async-generator>=1.9 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: sniffio in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting pyOpenSSL>=0.14\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from urllib3[secure]~=1.26->selenium) (3.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
      "Installing collected packages: sortedcontainers, outcome, wsproto, trio, pyOpenSSL, trio-websocket, selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 3.141.0\n",
      "    Uninstalling selenium-3.141.0:\n",
      "      Successfully uninstalled selenium-3.141.0\n",
      "Successfully installed outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sortedcontainers-2.4.0 trio-0.19.0 trio-websocket-0.9.2 wsproto-1.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U chromedriver-binary\n",
    "!pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:30:59.120206Z",
     "start_time": "2021-11-19T14:30:59.115502Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FileSystem' from 'toolbox' (/home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/toolbox/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_770/1732298019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#import toolbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystem\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageHandler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mih\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystemGUI\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FileSystem' from 'toolbox' (/home/christophearendt/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/toolbox/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "#import toolbox\n",
    "import toolbox\n",
    "from toolbox import FileSystem as fs\n",
    "from toolbox import ImageHandler as ih\n",
    "from toolbox import FileSystemGUI as gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:30:59.656242Z",
     "start_time": "2021-11-19T14:30:59.652981Z"
    }
   },
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--disable-extensions')\n",
    "chrome_options.add_argument('--profile-directory=Default')\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-plugins-discovery\")\n",
    "chrome_options.add_argument(\"--start-maximized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrapping tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:31:02.238167Z",
     "start_time": "2021-11-19T14:31:02.234286Z"
    }
   },
   "outputs": [],
   "source": [
    "# needed imports\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SEARCH_PAUSE_TIME = 4\n",
    "SCROLL_PAUSE_TIME = 4\n",
    "SLEEP_BETWEEN_INTERACTIONS = 1\n",
    "PATIENCE = 10\n",
    "sleeps = [1, 0.5, 1.5, 0.7, 0.8, 0.3, 1.1, 1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:31:02.648261Z",
     "start_time": "2021-11-19T14:31:02.644836Z"
    }
   },
   "outputs": [],
   "source": [
    "def scroll_to_end(wd):\n",
    "    \n",
    "    last_height = wd.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        wd.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = wd.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    return wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:31:03.378365Z",
     "start_time": "2021-11-19T14:31:03.373471Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_thumbnails(wd, base_image_path):\n",
    "\n",
    "    # collect all images html elements\n",
    "    image_links = wd.find_elements_by_class_name('rg_i.Q4LuWd')\n",
    "    # collect all images sources or vignettes\n",
    "    src_links = [image_links[i].get_attribute('src')for i in range(len(image_links))]\n",
    "    data_src_links = [image_links[i].get_attribute('data-src') for i in range(len(image_links))]\n",
    "\n",
    "    # actual collecting of vignettes for given search\n",
    "    for i,link in enumerate(tqdm(data_src_links)):\n",
    "        if link is None:\n",
    "            link = src_links[i]\n",
    "        \n",
    "        dest_path = f\"{base_image_path}_{i}.jpeg\"\n",
    "        try:\n",
    "            dest_path, _ =urllib.request.urlretrieve(link, dest_path)\n",
    "        except:\n",
    "            continue # continue to next row\n",
    "        time.sleep(np.random.choice(sleeps))\n",
    "    return wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:31:04.234104Z",
     "start_time": "2021-11-19T14:31:04.230286Z"
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_elements(driver, byType, byValue, maxWait: int):\n",
    "    \"\"\"\n",
    "    This method, takes properties of an element and looks for the element in page.\n",
    "    if the element is found, it will return the element else None is returned\n",
    "\n",
    "    Args:\n",
    "        driver ([webdriver]): [Selenium webdriver]\n",
    "        byType ([By Type]): [Which property we are using to get element. ex: By.XPATH or BY.Id]\n",
    "        byValue ([type]): [value of property]\n",
    "        maxWait (int): [wait for element to be found]\n",
    "\n",
    "    Returns:\n",
    "        [webdriverelement]: [identified element]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = WebDriverWait(driver, maxWait).until(\n",
    "            EC.presence_of_all_elements_located((byType, byValue)))\n",
    "        return element\n",
    "    except TimeoutException:\n",
    "        print(\"Element not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:31:05.000831Z",
     "start_time": "2021-11-19T14:31:04.994417Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_links_to_image(wd, path_to_backup_links=\"\", verbose=False):\n",
    "\n",
    "    image_urls = []\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "\n",
    "    # get all image thumbnail results\n",
    "    thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "    number_results = len(thumbnail_results)\n",
    "        \n",
    "    print(f\"Found: {number_results} search results.\")\n",
    "    print(f\">> Extracting links from {results_start}:{number_results}\")   \n",
    "    for img in  tqdm(thumbnail_results[results_start:number_results]):\n",
    "        # try to click every thumbnail such that we can get the real image behind it\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(SLEEP_BETWEEN_INTERACTIONS)\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        # extract image urls (avoid images from google cache)  \n",
    "        actual_images = wait_for_elements(wd, By.CSS_SELECTOR, 'img.n3VNCb', PATIENCE)\n",
    "        if (None != actual_images):\n",
    "            for actual_image in actual_images:\n",
    "                src_ = actual_image.get_attribute('src')\n",
    "                if src_ is not None and \\\n",
    "                   ('http' in src_ or 'http' in src_) and \\\n",
    "                  'encrypted-tbn0.gstatic.com' not in src_:\n",
    "                    if verbose is True:\n",
    "                        print(src_)\n",
    "                    image_urls.append(src_)\n",
    "                    if len(path_to_backup_links) > 0:\n",
    "                        with open(path_to_backup_links, 'a') as f:\n",
    "                            f.write(f\"{src_}\\n\")\n",
    "\n",
    "    return image_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual scrapping starts here ... chill out and come back later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:10:05.906691Z",
     "start_time": "2021-11-19T14:10:01.729131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER YOUR SEARCH CRITERIA:\n",
      "arc de triomphe\n"
     ]
    }
   ],
   "source": [
    "print('ENTER YOUR SEARCH CRITERIA:')\n",
    "quest = input();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T14:10:32.322582Z",
     "start_time": "2021-11-19T14:10:23.288603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER PATH TO IMAGES SEARCH OUTCOME:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gui' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_770/2601493948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ENTER PATH TO IMAGES SEARCH OUTCOME:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gui' is not defined"
     ]
    }
   ],
   "source": [
    "print('ENTER PATH TO IMAGES SEARCH OUTCOME:')\n",
    "dataset_directory = gui.gui_fname().decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-19T14:32:16.423Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "start = 1\n",
    "count = start\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options);\n",
    "driver.get(\"https://www.google.fr/imghp?hl=fr&ogbl\")\n",
    "\n",
    "# handling accept cookies for search: searching for 'accept' button\n",
    "ids = driver.find_elements_by_xpath('//*[@id]')\n",
    "for ii in ids:\n",
    "    if ii.tag_name == \"button\" and ii.text == \"J'accepte\":\n",
    "        ii.click()\n",
    "# trigger the actual search for the asana\n",
    "search = driver.find_element_by_tag_name(\"input\")\n",
    "search.clear()\n",
    "\n",
    "# create depositoty for scraping outcome\n",
    "dataset_path = f\"{dataset_directory}/{quest}\"\n",
    "if not fs.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)  \n",
    "\n",
    "# create file for storing original links to image\n",
    "dataset_links_backup_path = f\"{dataset_directory}/{quest}/{quest.replace(' ', '_')}.txt\"\n",
    "\n",
    "# trigger the search image engine here \n",
    "\n",
    "search.send_keys(quest)\n",
    "search.submit()\n",
    "\n",
    "# wait a notch\n",
    "time.sleep(SEARCH_PAUSE_TIME)\n",
    "\n",
    "# ----------------------------------------------------------------------------#\n",
    "# SCROLL TO THE BOTTOM OF THE SEARCH RESULTS till it displays \"display more\" -# \n",
    "# ----------------------------------------------------------------------------#\n",
    "driver = scroll_to_end(driver)\n",
    "print('Hit any key to continue')\n",
    "rr = input();\n",
    "\n",
    "# ---------------------------------------------------#\n",
    "# COLLECT IMAGE THUMBNAILS AND BACKUP LINKS TO .TXT -# \n",
    "# ---------------------------------------------------#\n",
    "\n",
    "if not os.path.exists(dataset_links_backup_path):\n",
    "    list_of_urls = collect_links_to_image(driver, dataset_links_backup_path, verbose=True)\n",
    "else:\n",
    "    list_of_urls = [line.strip() for line in open(dataset_links_backup_path, 'r')]\n",
    "    \n",
    "# ----------------------------------#\n",
    "# -    COLLECT IMAGES TO DATASET  - # \n",
    "# ----------------------------------#    \n",
    "print(f\">> Downloading dataset images for: {search}\")\n",
    "base_dest_path = dataset_links_backup_path\n",
    "#collect_thumbnails(driver, base_dest_path)\n",
    "\n",
    "i = 0\n",
    "issues = []\n",
    "\n",
    "for url in tqdm(list_of_urls):\n",
    "    file_ext_pattern = '\\.(?i)(jpe?g|png|gif|tif|cms|bmp|img|webp)'\n",
    "    match= re.findall(file_ext_pattern,(fs.file_extension(url)))\n",
    "    if match:\n",
    "        dest_path = f\"{base_dest_path}_{i}.{match[0]}\"\n",
    "        format_is_known = True\n",
    "    else:\n",
    "        dest_path = f\"{base_dest_path}_{i}\"\n",
    "        format_is_known = False\n",
    "        \n",
    "    try:\n",
    "        req = requests.get(url,timeout=5, headers={\"User-Agent\": \"Chrome/96.0.4664.55\"})\n",
    "        if req.status_code == 200:\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                f.write(req.content)\n",
    "            if format_is_known is False:\n",
    "                os.rename(dest_path,\n",
    "                            f\"{dest_path}.{ih.get_image_format(dest_path)}\")\n",
    "                \n",
    "        else: \n",
    "            issues.append({f\"{req.status_code}\":url})\n",
    "\n",
    "        \n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        issues.append({\"Http Error\": url})\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        issues.append({\"Error Connecting\": url})\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        issues.append({\"Timeout Erro\": url})\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        issues.append({\"RequestException\": url})  \n",
    "    except:\n",
    "        issues.append({\"Some other exception...\": url})\n",
    "        \n",
    "    time.sleep(np.random.choice(sleeps))\n",
    "    i+=1\n",
    "if len(issues) > 0:\n",
    "    print(f\">> Reporting issues for scrapping: {quest}\")\n",
    "    for d in issues:\n",
    "        print(f\" --- type: {list(d.keys())[0]} - link: {list(d.values())[0]}\") \n",
    "# try to end gracefully...\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "driver.quit()\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e1ed13cf40d3d13d8a766212910d0b415e7b08781604c28646004b6676c80ab"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
